{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path pattern for .edf files in all subdirectories of the Data directory\n",
    "path_pattern = os.path.join('..', '..', 'Data', 'Subj*', '*.edf')\n",
    "\n",
    "# Use glob to get all matching files\n",
    "edf_files = glob(path_pattern, recursive=True)\n",
    "\n",
    "# Remove all files that contain .md in it\n",
    "edf_files = [f for f in edf_files if '.md' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\..\\\\Data\\\\Subj1\\\\Alexis_Fast_EPOCX_218750_2024.05.20T15.14.17+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj1\\\\Alexis_Slow_EPOCX_218750_2024.05.13T13.51.48+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj10\\\\Livy_Fast_EPOCX_218750_2024.05.22T15.14.00+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj10\\\\Livy_Slow_EPOCX_218750_2024.05.16T12.01.27+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj11\\\\Nicole_Fast_EPOCX_218750_2024.05.15T12.15.16+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj11\\\\Nicole_Slow_EPOCX_218750_2024.05.20T11.20.47+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj12\\\\Ryan_Fast_EPOCX_218750_2024.05.20T16.03.17+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj12\\\\Ryan_Slow_EPOCX_218750_2024.05.13T17.27.48+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj13\\\\Sanni_Fast_EPOCX_218750_2024.05.22T13.12.31+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj13\\\\Sanni_Slow_EPOCX_218750_2024.05.15T14.29.45+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj14\\\\Sherryn_Fast_EPOCX_218750_2024.05.20T12.36.50+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj14\\\\Sherryn_Slow_EPOCX_218750_2024.05.13T15.14.14+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj15\\\\Vincent_Fast_EPOCX_218750_2024.05.22T11.19.45+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj15\\\\Vincent_Slow_EPOCX_218750_2024.05.15T11.47.51+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj16\\\\Yudi_Fast_EPOCX_218750_2024.05.15T13.19.26+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj16\\\\Yudi_Slow_EPOCX_218750_2024.05.20T13.09.03+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj2\\\\Andrea_Fast_EPOCX_218750_2024.05.14T15.29.13+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj2\\\\Andrea_Slow_EPOCX_218750_2024.05.20T11.45.39+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj3\\\\Bennett_Fast_EPOCX_218750_2024.05.13T14.24.51+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj3\\\\Bennett_Slow_EPOCX_218750_2024.05.20T15.37.12+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj4\\\\Cecil_Fast_EPOCX_218750_2024.05.20T10.51.54+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj4\\\\Cecil_Slow_EPOCX_218750_2024.05.15T13.53.05+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj5\\\\Edbert_Fast_EPOCX_218750_2024.05.27T15.17.06+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj5\\\\Edbert_Slow_EPOCX_218750_2024.05.14T14.27.08+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj6\\\\Essley_Fast_EPOCX_218750_2024.05.15T09.38.59+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj6\\\\Essley_Slow_EPOCX_218750_2024.05.27T10.32.20+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj7\\\\Joliem_Fast_EPOCX_218750_2024.05.21T13.18.33+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj7\\\\Joliem_Slow_EPOCX_218750_2024.05.14T13.23.47+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj8\\\\Justin_Fast_EPOCX_218750_2024.05.14T14.53.54+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj8\\\\Justin_Slow_EPOCX_218750_2024.05.21T13.45.19+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj9\\\\Leo_Fast_EPOCX_218750_2024.05.13T16.48.25+07.00.edf',\n",
       " '..\\\\..\\\\Data\\\\Subj9\\\\Leo_Slow_EPOCX_218750_2024.05.20T16.24.55+07.00.edf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating recording metadata dataframe: subject_id, pace, file_path\n",
    "baseline_metadata = []\n",
    "for file in edf_files:\n",
    "    # Extract the subject_id and pace from the file path\n",
    "    subject_id = int(file.split(os.sep)[-2].split('Subj')[-1])\n",
    "    pace = file.split(os.sep)[-1].split('_')[1]\n",
    "    baseline_metadata.append([subject_id, pace, file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = pd.DataFrame(baseline_metadata, columns=['subject_id', 'pace', 'file_path'])\n",
    "md.to_csv('baseline_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stimulus_recording(path):\n",
    "    raw_edf = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    with(open(path.replace('edf', 'json'))) as f:\n",
    "        data_json = json.load(f)\n",
    "    # Get recording start time\n",
    "    recording_start_time = pd.to_datetime(data_json['Markers'][0]['startDatetime'])\n",
    "\n",
    "    # Get start time from data_json with phase_name = 'instructions_task'\n",
    "    starttime_data = pd.to_datetime(data_json['Markers'][1]['endDatetime'])\n",
    "    endtime_data = pd.to_datetime(data_json['Markers'][2]['startDatetime'])\n",
    "\n",
    "    # Relative start time\n",
    "    relative_start_time = (starttime_data - recording_start_time).total_seconds()\n",
    "    relative_end_time = (endtime_data - recording_start_time).total_seconds()\n",
    "    \n",
    "    r = raw_edf.copy().crop(tmin=relative_start_time, tmax=relative_end_time)\n",
    "    \n",
    "    # Floor r duration to 60 seconds\n",
    "    r_duration = r.times[-1]\n",
    "    if r_duration > 60:\n",
    "        r.crop(tmin=0, tmax=60)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_get_data(path):\n",
    "    recording = get_stimulus_recording(path)\n",
    "    \n",
    "    # Select Channels in standard 10-20 format that EPOC X used\n",
    "    # AF3, AF4, F3, F4, FC5, FC6, T7, T8, P7, P8, O1, O2\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    recording.drop_channels([ch for ch in recording.ch_names if ch not in montage.ch_names])    \n",
    "    \n",
    "    # Setting montage (position of electrodes)\n",
    "    recording.set_montage('standard_1020')\n",
    "    \n",
    "    # Setting EEG reference\n",
    "    recording.set_eeg_reference('average', verbose = False)\n",
    "    \n",
    "    # Bandpass Filter at 1 - 50 Hz\n",
    "    recording.filter(l_freq = 1, h_freq = 50, verbose=False)\n",
    "    \n",
    "    return recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 weird defect datas\n",
    "defect_datas = ['..\\\\..\\\\Data\\\\Subj1\\\\Alexis_Slow_EPOCX_218750_2024.05.13T13.51.48+07.00.edf', '..\\\\..\\\\Data\\\\Subj3\\\\Bennett_Fast_EPOCX_218750_2024.05.13T14.24.51+07.00.edf']\n",
    "defect_record_fixed = []\n",
    "for path in defect_datas:\n",
    "    raw_edf = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "    with(open(path.replace('edf', 'json'))) as f:\n",
    "        data_json = json.load(f)\n",
    "    # Get recording start time\n",
    "    recording_start_time = pd.to_datetime(data_json['Markers'][0]['startDatetime'])\n",
    "\n",
    "    # Get start time from data_json with phase_name = 'instructions_task'\n",
    "    starttime_data = pd.to_datetime(data_json['Markers'][1]['endDatetime'])\n",
    "    endtime_data = pd.to_datetime(data_json['Markers'][5]['startDatetime'])\n",
    "\n",
    "    # Relative start time\n",
    "    relative_start_time = (starttime_data - recording_start_time).total_seconds()\n",
    "    relative_end_time = (endtime_data - recording_start_time).total_seconds()\n",
    "\n",
    "    recording = raw_edf.copy().crop(tmin=relative_start_time, tmax=relative_end_time)\n",
    "\n",
    "    # Floor r duration to 60 seconds\n",
    "    r_duration = recording.times[-1]\n",
    "    if r_duration > 60:\n",
    "        recording.crop(tmin=0, tmax=60)\n",
    "        \n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    recording.drop_channels([ch for ch in recording.ch_names if ch not in montage.ch_names])    \n",
    "    \n",
    "    # Setting montage (position of electrodes)\n",
    "    recording.set_montage('standard_1020')\n",
    "    \n",
    "    # Setting EEG reference\n",
    "    recording.set_eeg_reference('average', verbose = False)\n",
    "    \n",
    "    # Bandpass Filter at 1 - 50 Hz\n",
    "    recording.filter(l_freq = 1, h_freq = 50, verbose=False)\n",
    "    \n",
    "    defect_record_fixed.append(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = [read_and_get_data(edf) for edf in edf_files]\n",
    "raws[1] = defect_record_fixed[0]\n",
    "raws[18] = defect_record_fixed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating epochs of 2 seconds with 50% overlap\n",
    "duration = 2\n",
    "overlap = duration / 2\n",
    "epochs = [mne.make_fixed_length_epochs(r, duration=duration, overlap=overlap, preload=True, verbose=False) for r in raws]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 8 epochs: 2, 3, 10, 18, 19, 53, 54, 55\n",
      "Dropped 2 epochs: 54, 55\n",
      "Dropped 15 epochs: 5, 6, 7, 8, 16, 17, 18, 34, 35, 44, 45, 51, 52, 53, 57\n",
      "Dropped 34 epochs: 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22, 23, 25, 26, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52\n",
      "Dropped 19 epochs: 0, 1, 2, 9, 10, 11, 12, 13, 14, 15, 24, 32, 33, 34, 35, 38, 39, 51, 52\n",
      "Dropped 13 epochs: 1, 7, 8, 9, 12, 20, 21, 25, 31, 32, 38, 54, 55\n",
      "Dropped 15 epochs: 9, 10, 11, 16, 17, 21, 26, 27, 28, 30, 31, 32, 47, 48, 49\n",
      "Dropped 1 epoch: 7\n",
      "Dropped 2 epochs: 26, 27\n",
      "Dropped 8 epochs: 6, 20, 21, 45, 46, 47, 53, 54\n",
      "Dropped 30 epochs: 1, 2, 3, 5, 6, 7, 12, 17, 18, 19, 20, 22, 23, 24, 25, 34, 35, 36, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58\n",
      "Dropped 11 epochs: 15, 16, 17, 18, 20, 34, 35, 36, 37, 38, 58\n",
      "Dropped 5 epochs: 13, 14, 15, 16, 17\n",
      "Dropped 13 epochs: 5, 6, 7, 9, 10, 11, 12, 16, 17, 30, 31, 32, 33\n",
      "Dropped 4 epochs: 19, 30, 31, 54\n",
      "Dropped 3 epochs: 43, 44, 45\n",
      "Dropped 3 epochs: 5, 13, 14\n",
      "Dropped 12 epochs: 7, 15, 16, 17, 18, 19, 20, 33, 34, 41, 42, 50\n",
      "Dropped 5 epochs: 2, 3, 4, 5, 17\n",
      "Dropped 3 epochs: 0, 3, 4\n",
      "Dropped 11 epochs: 25, 26, 27, 28, 36, 37, 51, 53, 54, 55, 57\n",
      "Dropped 11 epochs: 8, 9, 10, 11, 22, 34, 35, 41, 42, 43, 44\n",
      "Dropped 9 epochs: 34, 35, 36, 37, 38, 39, 40, 55, 56\n",
      "Dropped 10 epochs: 11, 12, 23, 24, 28, 30, 44, 45, 46, 57\n",
      "Dropped 6 epochs: 2, 17, 18, 33, 34, 50\n",
      "Dropped 4 epochs: 1, 14, 35, 36\n",
      "Dropped 5 epochs: 1, 2, 24, 43, 49\n",
      "Dropped 4 epochs: 2, 3, 18, 19\n",
      "Dropped 7 epochs: 18, 19, 36, 37, 38, 41, 42\n",
      "Dropped 25 epochs: 6, 7, 8, 15, 16, 17, 19, 20, 25, 26, 35, 36, 37, 38, 39, 40, 49, 50, 51, 52, 53, 54, 56, 57, 58\n",
      "Dropped 18 epochs: 3, 4, 5, 12, 15, 16, 17, 18, 19, 20, 21, 22, 44, 45, 50, 55, 56, 58\n"
     ]
    }
   ],
   "source": [
    "# Autoreject\n",
    "from autoreject import AutoReject\n",
    "\n",
    "n_interpolate = [2, 3, 4]\n",
    "ar = AutoReject(n_interpolate=n_interpolate, verbose=False, random_state=42)\n",
    "\n",
    "epochs_ar = []\n",
    "reject_log = []\n",
    "for epoch in epochs:\n",
    "    ar.fit(epoch)\n",
    "    e, r = ar.transform(epoch, return_log = True)\n",
    "    epochs_ar.append(e)\n",
    "    reject_log.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Export epochs_ar\n",
    "for i, e in enumerate(epochs_ar):\n",
    "    directory = f\"../../CleanedEpochs/Baseline/Subj{md['subject_id'][i]}/\"\n",
    "    filename = f\"{directory}{md['pace'][i]}-epo.fif\"\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Save the epoch\n",
    "    e.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
